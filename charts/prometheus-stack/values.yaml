global:
  ingress:
    enabled: true
    host:
    paths:
      prometheus: /prometheus
      grafana: /grafana
      alertmanager: /alertmanager
    annotations:
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.middlewares: routing-oidc-forward-auth@kubernetescrd

prometheusStack:
  kubeProxy:
    enabled: false
  kubeScheduler:
    enabled: false
  kubelet:
    enabled: true
  nameOverride: prometheus-stack

  crds:
    enabled: false

  kubeControllerManager:
    enabled: false

  defaultRules:
    create: true
    disabled:
      KubeClientCertificateExpiration: true
      NodeClockNotSynchronising: true #CCE does not give access to that
      KubeletDown: true  #CCE does not give access to that
      InfoInhibitor: true #It is spamming
    rules:
      alertmanager: true
      etcd: true
      general: true
      k8s: true
      kubeApiserver: true
      kubeApiserverAvailability: true
      kubeApiserverError: true
      kubeApiserverSlos: true
      kubePrometheusGeneral: true
      kubePrometheusNodeAlerting: true
      kubePrometheusNodeRecording: true
      kubernetesAbsent: true
      kubernetesApps: true
      kubernetesResources: false #We need to customize the CPUThrottlingHigh Alert and increase the CPU to X; see also kubernetes-resources.yaml line 151
      kubernetesStorage: true
      kubernetesSystem: true
      kubeStateMetrics: true
      network: true
      node: true
      prometheus: true
      prometheusOperator: true
  prometheus:
    prometheusSpec:
      externalUrl: https://{{$.Values.global.ingress.host}}{{$.Values.global.ingress.paths.prometheus}}
      routePrefix: "/prometheus"
      podMonitorSelector: # note to future self: matchLabels: null is only necessary for prometheus 63.1.0, it is not necessary in >= 64.x
        matchLabels: null
      serviceMonitorSelector:
        matchLabels: null
      probeSelector:
        matchLabels: null
      retention: 1y # 365 days
      resources:
        requests:
          memory: "2255Mi"
          cpu: "60m"
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                # Depends on how many metrics & applications you are tracking, but we average out at about 5-6GB per 10 days
                storage: 250G

  additionalPrometheusRulesMap:
    uptime-monitoring:
      groups:
        - name: uptime-monitoring
          rules:
            - alert: ServiceIsDown
              annotations:
                description: 'Uptime of {{ $labels.instance }} is reporting down for 3 consecutive checks.'
                summary: '{{ $labels.instance }} not reachable'
              expr: >
                sum by (instance) (sum_over_time(probe_success{}[90s])) < 1
              labels:
                severity: critical

  prometheusOperator:
    enabled: true
    tls:
      enabled: false
    admissionWebhooks:
      enabled: false
      patch:
        enabled: false
  grafana:
    sidecar:
      dashboards:
        enabled: true
        folderAnnotation: k8s-sidecar-target-directory
        provider:
          foldersFromFilesStructure: true
    serviceMonitor:
      path: "/grafana/metrics"
    # -- Required
    adminPassword:
    grafana.ini:
      server:
        serve_from_sub_path: true
        root_url: https://{{$.Values.global.ingress.host}}{{$.Values.global.ingress.paths.grafana}}
      auth:
        disable_login_form: false
      auth.basic:
        enabled: true
      security:
        disable_initial_admin_creation: false

  alertmanager:
    config:
      global:
        resolve_timeout: 5m
        slack_api_url: "http://myhost.local"
      route:
        group_by: [ 'job' ]
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        routes:
          - receiver: 'null'
            matchers:
              - alertname="Watchdog"
          - receiver: 'slack'
            group_by: [ '...' ]
            group_wait: 1s
            group_interval: 1s
            repeat_interval: 30m
            matchers:
              - severity="critical"
          - receiver: 'slack'
      receivers:
        - name: 'null'
        - name: 'slack'
          slack_configs:
            - channel: 'infrastructure'
              link_names: true
              send_resolved: true
              title_link: '{{ (index .Alerts 0).GeneratorURL }}'
              title: '{{ (index .Alerts 0).GeneratorURL | reReplaceAll "(.*)prometheus(.*)" "$1"}}  {{ (index .Alerts 0).Annotations.summary}}'
              text: >-
                {{ range .Alerts }}
                  *Description:* {{ .Annotations.description }}
                  *Details:*
                  {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                  {{ end }}
                {{ end }}
      templates:
        - /etc/alertmanager/config/*.tmpl
    alertmanagerSpec:
      externalUrl: https://{{$.Values.global.ingress.host}}{{$.Values.global.ingress.paths.alertmanager}}
      routePrefix: "/alertmanager"
      resources:
        requests:
          memory: "100Mi"
          cpu: "5m"

# Kyverno Policy Exception
policyException:
  enabled: true

blackboxExporter:
  enabled: true # Chart dependency condition
  fullnameOverride: blackbox-exporter
  secretConfig: true
  # strategy: Recreate ensures that we do not have multiple pods reporting on the same endpoint during a rollout.
  # This does result in a brief downtime for the uptime monitoring but reduces the metric aggregation requirements on the dashboard and reduces the chances of inaccurate probe measurements.
  strategy:
    type: Recreate
    rollingUpdate: null
  extraArgs:
    [ "--log.level=warn", "--log.format=json" ]
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      memory: 100Mi
  config:
    modules:
      http_2xx:
        prober: http
        timeout: 5s
        http:
          valid_http_versions: [ "HTTP/1.1", "HTTP/2.0" ]
          preferred_ip_protocol: "ip4"
          valid_status_codes: [ ]  # Defaults to 2xx
      #http_2xx_oauth_client_credentials:
      #  prober: http
      #  timeout: 5s
      #  http:
      #    valid_http_versions: [ "HTTP/1.1", "HTTP/2.0" ]
      #    preferred_ip_protocol: "ip4"
      #    valid_status_codes: [ ]  # Defaults to 2xx
      #    oauth2:
      #      client_id: "SET_ME"
      #      client_secret: "SET_ME"
      #      token_url: "SET_ME"
      #      endpoint_params:
      #        grant_type: "client_credentials"
      dns_baseline:
        prober: dns
        timeout: 5s
        dns:
          preferred_ip_protocol: "ip4"
          query_name: "google.com" # domain to ask the DNS Server
          query_type: "A"
          valid_rcodes:
            - NOERROR

  securityContext:
    seccompProfile:
      type: "RuntimeDefault"
  serviceMonitor:
    enabled: false
    selfMonitor:
      enabled: true
    targets:

# Load dashboard(s) for the blackbox uptime monitor
dashboards:
  enabled: true

uptimeMonitors:
  ## If true, a ServiceMonitor CRD is created for a prometheus operator
  ## https://github.com/coreos/prometheus-operator for each target
  ##
  enabled: true

  # Default values that will be used for all ServiceMonitors created by `targets`
  defaults:
    additionalMetricsRelabels: {}
    additionalRelabeling: []
    labels: {}
    interval: 30s
    scrapeTimeout: 30s
    honorTimestamps: true
    module: http_2xx
    jobName: "prometheus-stack"
  ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
  scheme: http
  ## path: HTTP path. Needs to be adjusted, if web.route-prefix is set
  path: "/probe"
  ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
  ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
  tlsConfig: {}
  bearerTokenFile:

  defaultTargets:
    - name: baseline
      module: dns_baseline
      url: "8.8.8.8" # DNS Server to ask

  # Add your targets here
  # see defaults for possible parameters
  targets: []
    # - name: my-target
    #   url: https://app.example.com/
