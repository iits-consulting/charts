apiVersion: v2
name: ollama
description: |
  Get up and running with large language models locally.
  
  ## Installing the Chart with iits ArgoCD

  ```yaml
    ollama:
      namespace: ollama
      repoURL: "https://charts.my-domain.com"
      targetRevision: "0.7.4"
      valueFile: "value-files/ollama/values.yaml"
  ```


  value-files/ollama/values.yaml

  ```yaml
  ollama:
    ollama:
      gpu:
        enabled: "true"
    ingress:
      host: "ollama.my-domain.com"
      annotations:
        # Adds the oidc proxy upfront
        traefik.ingress.kubernetes.io/router.middlewares: "ollama-oidc-forward-auth-ollama@kubernetescrd, ollama-strip-prefix-ollama@kubernetescrd"
  webui:
    env:
      OLLAMA_API_BASE_URL: "https://ollama.my-domain.com/api"
    ingress:
      annotations:
        # Adds the oidc proxy upfront
        traefik.ingress.kubernetes.io/router.middlewares: "ollama-oidc-forward-auth-ollama@kubernetescrd"
      host: "ollama.my-domain.com"
  middleware:
    env:
      OLLAMA_URL: "http://ollama:11434"
      WEAVIATE_URL: "http://weaviate:80"
    ingress:
      annotations:
        # Adds the oidc proxy upfront
        traefik.ingress.kubernetes.io/router.middlewares: "ollama-oidc-forward-auth-ollama@kubernetescrd"
      host: "ollama.my-domain.com"
  ```
type: application
version: 0.8.0
kubeVersion: "^1.16.0-0"
home: https://ollama.ai/
icon: https://ollama.ai/public/ollama.png
keywords:
  - ai
  - llm
  - llama
  - mistral